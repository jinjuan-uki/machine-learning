# PCA的数学推导
### 原理介绍
PCA算法是一个用途非常广的降维手段，这种方法其实是一种特征提取方法（feature extraction），是对原始特征进行变化之后的降维压缩，需要注意的是，这并不是特征选择（feature selection）。PCA的基本思想就是，寻找一组正交基底，使得原始数据的空间发生变化，使得在新的空间的各个维度上方差最大化（通常认为，特征方差越大的特征，包含的信息越重要），总体可以概括为：降低特征空间维度；消除原有特征之间的相关度，减少数据信息的冗余。PCA需要解决的问题是，找到一组k维正交基底，使得新的特征内部方差最大，特征间相关程度最小，所以从这个角度上来看，PCA也是一个优化问题，下面的数学推导就将按照优化的步骤进行。
### 数学推导
假设我们有一个样本集，每个样本的特征为N，样本容量为p，那么我们用一个向量来表示这个样本集，每一个列向量为一个样本，每一行则为一个特征：
$$
X = 
    \begin{pmatrix}
    x_1,x_2,...,x_p
    \end{pmatrix} =
    \begin{pmatrix}
    x_11,x_21,...,x_p1 \\
    x_12,x_22,...,x_p2 \\
    ......\\
    x_{1N},{x_2N},...,{x_pN}
    \end{pmatrix}
$$
显然，向量X是一个N*p的矩阵，每一个$x_i$为一个N*1的矩阵。
我们希望获得一个M个N维分量A，A的形状为M*N，与$x_i$相乘之后，即可将$x_i$变化为一个M维的向量，实现降维的目的：
$$
A=
    \begin{pmatrix}
    a_1 \\
    a_2 \\
    ... \\
    a_{M} \\
    \end{pmatrix} 
$$

其中，每一个$a_i$都是一个1*N维的行向量。接下我们对原始数据进行去中心化处理，这样的目的是为了消除量纲的影响，均值$\bar{X}$的计算如下：
$$
\bar{X}=\frac 1pb\sum_{i=1}^{p}{x_i}
$$
最后，我们得到的降维结果为：
$$
Y=
A(X-\bar{X})=
    \begin{pmatrix}
    a_1 \\
    a_2 \\
    ... \\
    a_M \\
    \end{pmatrix}
    \begin{pmatrix}
    x_1,x_2,...,x_p
    \end{pmatrix}=
    \begin{pmatrix}
    a_1(x_1-\bar{X}),a_1(x_2-\bar{X}),...,a_1(x_p-\bar{X}) \\
    a_1(x_2-\bar{X}),a_1(x_2-\bar{X}),...,a_1(x_2-\bar{X}) \\
    .... \\
    a_M(x_1-\bar{X}),a_M(x_2-\bar{X}),...,a_M(x_p-\bar{X}) \\
    \end{pmatrix}
$$

变换后的Y为一个M*p的矩阵，每一行代表一个变换后的特征。为了方便表示，进一步简化Y矩阵为：
$$
Y = 
    \begin{pmatrix}
    Y_1 \\
    Y_2 \\
    ... \\
    Y_M \\
    \end{pmatrix} = 
    \begin{pmatrix}
    y_{11},y_{12},...,y_{1p} \\
    y_{21},y_{22},...y_{2p} \\
    ... \\
    y_{M1},y_{M2},...y_{Mp} \\
    \end{pmatrix}
$$

现在引出我们的优化问题，上面说了，我们希望经过新的坐标系变化后的特征内部方差最大化，因此我们取第一个分量$Y_1$来进行优化：
$$
\max{\sum_{i=1}^p{(y_{1i}-\bar{Y_1})^2}}
$$

其中
$$
\bar{Y_1}=\frac 1p\sum_{i=1}^pa_1{(x_{i}-\bar{X}})\\= \frac {a_1}p\sum_{i=1}^pa_1{(x_{i}-\bar{X}})\\=\frac {a_1}p\sum_{i=1}^p{(x_{i}-\bar{X}})\\=\frac {a_1}p(\sum_{i=1}^px_i-\sum_{i=1}^p\bar{x})\\=0
$$
所以，目标函数变为了：
$$
\max{\sum_{i=1}^py_{1i}^2}=\sum_{i=1}^p(a_1(x_i-\bar{X}))^2
= \sum_{i=1}^p[a_1(x_i-\bar X)][a_1(x_i-\bar X)]^T\\=a_1\sum_{i=1}^p[(x_i-\bar{X})(x_i-\bar{X})^T]a_1^T\\=a_1\sum a_1^T
$$

其中，$\sum$为协方差矩阵。上式表明了一个事实：目标函数和协方差矩阵有关，下面给出进一步的优化问题：
$$
\max a_1\sum a_1^T \\
 a_1a_1^T=||a_1||=1
$$

约束条件要求分量的模为一，是因为正交基底的要求，便于后续计算。对于该优化问题，使用拉格朗日乘数法进行求解：
$$
E(a_1)=a_1\sum a_1^T-\lambda_1(a_1a_1^T-1)
$$

对a1求一阶导数，令其为零：
$$
\frac {\partial E}{\partial a_1}=2(\sum a_1^T-\lambda_1a_1^T)^T=0\\
\sum a_1^T=\lambda_1a_1^T
$$

由上式可推知，$a_1^T$是$\sum$的特征向量，而$\lambda$是其特征值，带入目标函数：
$$
\max a_1\sum a_1^T=\max a_1(\sum a_1^T)=a_1\lambda_1 a_1^T=\lambda_1 a_1a_1^T=\lambda_1 
$$

所以，$\lambda_1$只需要取得最大特征根即可实现最优化。
算完了a1，那么我们就要算a2了，a2是否还能按照原先的方法求出来呢？答案是否定的，因为如果还按照刚才的方法，显然我们的a1与a2就会出现无限的逼近了，这就不符合我们的初衷了，所以我们要求a1与a2正交。重新定义优化问题：
$$
\max a_2\sum a_2^T \\
 a_2a_2^T=||a_2||=1 \\
 a_2a_1^T=a_1a_2^T=0
$$

同样使用拉格朗日：
$$
E(a_2)=a_2\sum a_2^T-\lambda_2(a_2a_2^T-1)-\beta a_1a_2^T
$$

$$
\frac {\partial E}{\partial a_2}=(2\sum a_2^T-2\lambda_2a_2^T-\beta a_1^T)^T=0
$$

下面需要证明，beta为0：
$$
2\sum a_2^T-2\lambda_2a_2-\beta a_1=0\\
$$

由于$\sum$为对称阵：
$$
(2\sum a_2^T-2\lambda_2a_2-\beta a_1)a_1^T\\= 
2a_2\sum a_1^T-2\lambda_2a_2a_1^T-\beta a_1a_T\\= 
2a_2\lambda_1a_1^T-2\lambda_2a_2a_1^T-\beta a_1a_1^T\\ = 2a_2\lambda_1a_1^T-2\lambda_2a_2a_1^T-\beta a_1a_1^T \\ =0
$$

前两项，由于正交的原因，都为0，所以beta也要为0，才能使得等式成立，故得证。
所以：
$$
\max a_2 \sum a_2^T \\
a2_2a_2^T=1\\
\sum a_2^T=\lambda_2a_2^T \\
$$

a2同样是特征向量，由于最大的特征根已经被a1取走了，故此处的lambda只能取第二大的特征根了，a2对应的是第二大特征根对应的特征向量。同样的，第三个第四个分量，也是一样的方法去求得，对应的特征根和特征向量也是第三大、第四大的，那么我们的PCA推导就正式结束了。

### 算法步骤
经过上文的推导，就会发现，PCA和我们的协方差矩阵的特征向量息息相关，我们只需要求出特征向量，特征根，排好顺序，取出来相对应降维后的特征数的前几个特征向量，就完成了我们的PCA全过程，整个流程总结如下：

1. 求协方差矩阵
2. 求协方差矩阵的特征值，并按照从大到小的顺序排列
3. 归一化特征向量
4. 取出降维数目的前几个特征向量
5. 计算降维结果

值得一提的是，PCA和神经网络中的自动编码器原理相近，可以看作线性的自动编码器。自动编码器相当于一个三层经典神经网络，隐层神经元小于输入的个数，隐层相当于是一个信息瓶颈，对输入进行压缩，PCA和其原理是一致的，只不过是线性的罢了，在某些问题上，这两种方法不分伯仲，有兴趣的小伙伴可以了解一下。
